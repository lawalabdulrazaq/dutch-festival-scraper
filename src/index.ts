import { validateConfig, config } from './config/config';
import { getScrapers } from './scrapers';
import { supabaseService } from './services/supabase.service';
import { httpService } from './services/http.service';
import { FestivalEvent } from './types/event.types';
import { removeDuplicates } from './utils/string.utils';
import { logger } from './utils/logger';

/**
 * Main scraper execution
 */
async function runScraper(): Promise<void> {
  logger.info('ðŸš€ Starting Dutch Festival Scraper...');
  
  // Validate configuration
  if (!validateConfig()) {
    process.exit(1);
  }

  try {
    // Load processed events from database
    logger.info('ðŸ“¦ Loading processed events from database...');
    const processedKeys = await supabaseService.getProcessedEvents();
    logger.info(`Found ${processedKeys.size} already processed events`);

    // Get all enabled scrapers
    let scrapers = getScrapers();

    // Support running a single scraper for fast testing via env var `SCRAPE_ONLY`
    const scrapeOnly = process.env.SCRAPE_ONLY?.trim();
    if (scrapeOnly) {
      const wanted = scrapeOnly.split(',').map(s => s.trim().toLowerCase()).filter(Boolean);
      scrapers = scrapers.filter(s => wanted.includes((s as any).config.name.toLowerCase()));
      logger.info(`ðŸ§ª SCRAPE_ONLY active, running ${scrapers.length} scraper(s): ${wanted.join(', ')}`);
    } else {
      logger.info(`ðŸ•·ï¸  Running ${scrapers.length} scrapers...`);
    }

    // Run all scrapers in parallel
    const results = await Promise.all(
      scrapers.map(scraper => scraper.execute())
    );

    // Collect all events
    let allEvents: FestivalEvent[] = [];
    results.forEach(result => {
      if (result.success) {
        allEvents = allEvents.concat(result.events);
      }
    });

    logger.info(`ðŸ“Š Total events collected: ${allEvents.length}`);

    // Remove duplicates
    allEvents = removeDuplicates(allEvents);
    logger.info(`ðŸ“Š After deduplication: ${allEvents.length} unique events`);

    // Filter out already processed events
    const newEvents = allEvents.filter(event => !processedKeys.has(event.sleutel));
    logger.info(`âœ¨ New events to send: ${newEvents.length}`);

    if (newEvents.length === 0) {
      logger.info('âœ… No new events found. Scraping complete.');
      return;
    }

    // Send new events to client endpoint
    logger.info(`ðŸ“¤ Sending ${newEvents.length} new events to endpoint...`);
    
    // Log first event structure for debugging
    if (newEvents.length > 0) {
      logger.info('ðŸ“‹ First event structure:');
      logger.info(JSON.stringify(newEvents[0], null, 2));
    }
    
    const sentCount = await httpService.sendEvents(newEvents);
    logger.success(`âœ… Successfully sent ${sentCount}/${newEvents.length} events`);

    // Save processed event keys to database
    if (sentCount > 0) {
      logger.info('ðŸ’¾ Saving processed events to database...');
      const sentEventKeys = newEvents.slice(0, sentCount).map(e => e.sleutel);
      const savedCount = await supabaseService.saveProcessedEvents(sentEventKeys);
      logger.success(`âœ… Saved ${savedCount} processed event keys`);
    }

    // Summary
    logger.info('');
    logger.info('â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•');
    logger.info('ðŸ“Š SCRAPING SUMMARY');
    logger.info('â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•');
    results.forEach(result => {
      const status = result.success ? 'âœ…' : 'âŒ';
      logger.info(`${status} ${result.source}: ${result.events.length} events`);
    });
    logger.info(`ðŸ“Š Total collected: ${allEvents.length}`);
    logger.info(`âœ¨ New events: ${newEvents.length}`);
    logger.info(`ðŸ“¤ Sent: ${sentCount}`);
    logger.info('â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•');
    logger.info('');

  } catch (error) {
    logger.error('Fatal error during scraping', error);
    process.exit(1);
  }
}

/**
 * Schedule recurring execution
 */
function scheduleRecurring(): void {
  const intervalMs = config.scraper.intervalHours * 60 * 60 * 1000;
  
  logger.info(`â° Scheduling scraper to run every ${config.scraper.intervalHours} hours`);
  
  // Run immediately
  runScraper().catch(error => {
    logger.error('Scheduled scraper failed', error);
  });

  // Then run on schedule
  setInterval(() => {
    logger.info('â° Scheduled run starting...');
    runScraper().catch(error => {
      logger.error('Scheduled scraper failed', error);
    });
  }, intervalMs);
}

// Run based on command line argument
const args = process.argv.slice(2);

if (args.includes('--once')) {
  // Run once and exit
  runScraper()
    .then(() => process.exit(0))
    .catch(() => process.exit(1));
} else if (args.includes('--schedule')) {
  // Run on schedule
  scheduleRecurring();
} else {
  // Default: run once
  runScraper()
    .then(() => process.exit(0))
    .catch(() => process.exit(1));
}